\section{Introduction}\label{sec:intro}

% why feedback loops exist/are important (cite Joe’s shit in the past 3 years, blinkdb, etc.). highlight domain specificity. Large variety of data cleaning tasks

%
% Genereal comment that data cleaning is important:
%
Dealing with dirty data is a fundamental problem in modern data-driven applications -- without 
ensuring that a dataset is sufficiently error free, derived results (e.g., machine learning models or
analyst queries) can potentially contain significant errors that result in poor buisiness decisions that
the analyst is not even aware about!
Due to these risks of directly using dirty datasets, analysts easily spend 80\% or more of their analysis 
time~\cite{kandelvast} trying to identify and understand the data errors, while concurrently building custom scripts 
or large data cleaning pipelines in order to manage and fix the errors.  


One reason why removing data errors is so time consuming is that
the types of errors, the appropriate cleansers, as well as the
extent that they need to be fixed, is highly domain-specific.  For
example, \ewu{examples}.  In addition these could originate from
an internal or external data source, or be introduced as part of
earlier data curation steps~\cite{}.  Thus, as the size and number
datasets used for analysis continue to grow, the ability to rapidly
clean the data is increasingly critical.


%The prevalance of data cleaning systems in both the research and industrial communities --
%Corleone does blah, XXX addresses blah. Nadeef does blah -- speak to the importance of a
%data cleaning framework as part of the modern big data ecosystems. \ewu{include open access of data in argument?} \jn{We also need to take a look at data cleaning systems in industry. }


% why existing systems suck aka related work
% 1. have slow feedback loops (dataset-dependence, …)
% 2. solve very specific data-cleaning tasks
The problem of data cleaning is not new -- data cleaning systems have been
explored by both the research and industry communities since the beginning of data management.
Early on, extract-transform-load (ETL) systems~\cite{informatica,talend}\ewu{need refs}
were designed to be labor intensive -- developers manually wrote data cleaning rules which were executed as 
long batch jobs\ewu{is there a hadoop-era ETL ref?}.  Projects such as Wrangler~\cite{wrangler,trifacta} and
OpenRefine~\cite{googlerefine} observed that data cleaning is highly iterative.  
These systems introduced user-oriented spreadsheet-style interfaces that 
enable the user to inspect a sample of the dataset, compose data cleaning sequences using a 
direct manipulation interface, and apply these sequences over the full dataset.   
More recently, a number of crowd-based~\cite{corleone,tamr}
systems have been proposed to reduce the burden of performing the manual cleaning tasks from the data cleaning analyst 
by utilizing crowd workers on systems such as Mechanical Turk or ODesk~\cite{argonaut}.

Unfortunately, these systems will be challenged to scale towards increasing varieties of data sources
and data volumes due to two key limitations.  First, the batch oriented systems result in a very slow feedback loop 
that inhibits the user's ability to rapidly prototype different data cleaning solutions (e.g., tweak and 
try different similarity metrics or sampling techniques based on current data cleaning results).
Second, the existing interactive and iterative systems are designed for very specific cleaning tasks 
(e.g., deduplication~\cite{corleone}\ewu{add more examples}).  Each system may not apply, nor be sufficient, across
a large varietiy of data sources or analysis domains.
These existing limitations suggest the need for a system that is both general enough to
adapt to a wide range of data cleaning applications, scalable to large datasets, 
and that natively supports high-feedback interactions to enable rapid data cleaning iterations.

To this end, we are currently building \sys, a scalable data
cleaning framework designed to facilitate a tight feedback loop between
the user and the data cleaning system.  Our goal is to facilitate
a highly interactive ``rinse-and-repeat'' form of data cleaning
where the user can easily prototype a data-cleaning workflow, quickly
inspect the quality of the cleaned results, and easily change the
pipeline or the parameters of the cleaning operators on the fly.
Data cleaning domain specific language, optimizations, and careful interaction with crowds.
Describe engineering challenges, and core operators that enable optimization.

\sys introduces a number of interesting research challenges, which we characterize into two categories:
{\it instance feedback loops} and 
{\it hot swapping support}.  The former focuses on techniques that aggressively reduce
end-to-end data cleaning execution latency (including crowdsourcing latency), and
sampling and introspection mechanisms to quickly examine the progress and quality of a 
data cleaning pipeline.  The latter introduces mechanisms to reconfigure data cleaning
operators and parameters, as well as a recommender that allows \sys, with little user input,
to self-optimize the data cleaning plan for latency or quality.  For some cleaning plans, \sys can additionally provide equivalence guarantees.

In this demonstration, we will illustrate how \sys can be used to implement and execute
\ewu{several existing data cleaning pipelines}.   We will provide a web-based dashboard that
can be used to specify data-cleaning pipelnies; compile them into physical cleaning plans; and finally inspect, configure, or swap 
particular physical operators for alternative implementations.  This interaction allows the crowd to 
act as a human optimizer and inspect the effects of different physical plans.  Users can then
execute the pipeline over a live crowd that uses the audience as workers, or a simulated crowd
that uses pre-collected crowd responses.    The dashboard also provides a live inspection
interface to view the status of the cleaning plan as it executes \ewu{Describing how to do this may be interesting.}


%Our contributions/requirements
%different ways to tighten the feedback loop:
%end-to-end latency/cost (operator optimization)
%looking versus touching
%Adding introspection (more points of observation)
%hot-swapping (more points of changing plans)
%We have built an end-to-end data cleaning framework with these requirements in mind. (... things we do …) (... engineering contributions …).
%In this demonstration, we highlight the benefits of improving feedback loops for data cleaning using X datasets by optimizing a data cleaning pipeline for one data set/cleaning task, then quickly fitting the pipeline to another dataset.


\if{0}

\jn{Honestly, I didn't quite buy declarativity of the system. In my opinion, data cleaning is so domain specific. It's hard to make it declarative. For a given domain, people may need to write their own data cleaning system. There is a lack of a data cleaning framework that they can build based on. This motivates us to develop such framework. 


We analyze a large variety of domain specific data cleaning systems, and identify several key components: declarative data cleaning operators (e.g., similarity joins), active learning, and crowd/expert sourcing platforms that they require. In our framework, we abstract these components, and implement them in a general way. 

We mainly address two challenges: extensibility and scalability. For the former one, we came up with a nice data-cleaning pipeline API, which people can easily use to compose their own data cleaning tasks. For the latter one, we address it in two aspects: Sampling + Asynchrony.}

\ewu{That's fair, will need to address why a framework is necessary and what benefits it provides.  I think a framework is the correct pitch, hard to sell a set of operators.  Are the above challenges -- extensibility and scalability -- actually difficult?  Worried it's straightforward application of existing techniques.}



In contrast, our work is based on the observation that the majority of data cleaning workflows
can be decomposed into a small set of logical operations (in addition to traditional database operators):
filter based on constraints, extract new fields from existing data, and a similarity join to match
similar or duplicate records. \ewu{quickly validate why this observation holds.} \jn{Yes! I also found that Sec 2.3 has more operators than you describe here.}  
By designing a system around these core operators, we can provide a vast library of physical  
data cleaning operators that span the range of algorithmic, machine learning, and human computation-based
implementations that are necessary practical data cleaning pipelines.   \ewu{Describe live inspection as 
a core feature or is it too easy?}

Designing such a system requires tackling several design challenges:

\begin{enumerate}
\item Speed
\item Quality
\item API Design/extensibilty
\end{enumerate}



We have implemented an initial version of \sys on top of the AMPLab Spark stack, which provides us 
with access to its advanced distributed processing and machine learning features.  Our goal for the current
version is to implement the core mechanisms for declarative specification of the
data cleaning pipeline, solidify the API design, and incle support for, and implementations of,
multiple classes of physical data cleaning operators.


\fi



\if{0}

Cleaning, pre-processing, and formatting data is a required first step in any data analytics pipeline.
However, despite this importance, large-scale data analytics platforms such as Spark or Hadoop lack integrated data cleaning frameworks.
There are a few challenges in building a general purpose data cleaning framework: (1) data cleaning is often
domain specific and requires specialized software targeted at one or a handful of data sources \cite{wang1999sample}, (2) data cleaning is often 
expensive as it increasingly involves human effort via crowdsourcing or experts \cite{DBLP:conf/sigmod/GokhaleDDNRSZ14}, and (3) learning how to clean dirty data from examples
is often hard without a greatly restricted set of operators \cite{DBLP:conf/uist/GuoKHH11}.

We address this problem in \projx by designing a Spark library of composable and scalable data cleaning primitives.
\projx abstracts the logical data cleaning operators: Extraction, Similarity Join, Filtering, from the physical implementation i.e, Rule, Crowd, or Machine Learning.
We interface these primitives through a DSL with which a user can build data cleaning operators that suit their needs.
\projx provides transparent optimizations for each of the components and their composition.
In this demonstration proposal, we present \projx and highlight some of its key features.
While there are many existing systems that do one aspect of data cleaning and transformation (e.g Entity Resolution or Extraction), 
many real world data cleaning tasks have multiple types of errors.
Composing disparate systems can lead to complex code and inefficiencies at scale.
With \projx, we hope to design a set of optimized composable primitives that span a large space of data cleaning tasks.

The first key feature of \projx is that it provides optimized distributed implementations 
of the physical data cleaning operators.
For example, a key step in many deduplication algorithms is a Similarity Join which finds all pairs of records that are within some similarity threshold.
A naive implementation of a Similarity Join would apply a similarity function to all pairs of records.
However, in \projx, we provide optimized implementations of certain common similarity functions (e.g Jaccard, Edit Distance, etc.) that allow for 
a combined broadcast join and prefix filtering which intelligently skips pairs of records using a broadcasted inverted index.

Another feature of \projx is managing the latency and the scale problems of crowd-based data cleaning. 
Crowdsourcing is increasingly prevalent in data cleaning, and \projx provides physical crowd-based implementations
of the logical operators.
However, crowds work at a different latency and scale point in comparison to distributed analytics platforms.
To address the latency problem, we build asychrony into the system.
The user can query intermediate results at any time as crowd responses stream in.
To address the scale issue, \projx provides sampling primitives.
The glue that ties all of the crowd components together is a Machine Learning technique called Active Learning.
As we collect more and more crowd responses, we learn a model that predicts these responses to apply it on the uncleaned data.
Active Learning selects the most informative questions to ask the crowd.

Finally, \projx provides an approximate query processing (AQP) framework.
With slow asynchronous data cleaning algorithms as in crowdsourcing, we need 
to define clear semantics for the intermediate results.
Our AQP framework uses the algorithms proposed in \cite{wang1999sample}, to estimate and bound early results.
It is also common for data scientists to prototype expensive data cleaning pipelines on samples and AQP allows quick evaluation of
aggregate query results on a cleaned sample.

\subsection{Demonstration Scenario}
\reminder{TODO}

\fi




\if{0}
Consider for example, the ability to rapidly understand the types of errors that are present, as well as prevalance of 
these errors is cruicial.



Before an organization can use a new dataset as part of their analysis pipeline
(e.g., to build complex learning models or answer analyst queries)
the errors in the dataset need to be removed in order to ensure accurate conclusions.  

Modern data-driven organizations rely on the ability to ingest and generate large data sets from 
disparate sources, and combine the data together to build complex models or answer analytical questions.  
For example, a restaurant review website may collect restaurant listings by scraping data from webpages or purchasing them from external sources, and
restaurant visitation information for sources such as OpenTable or FourSquare, and aggregate the data to
model user eating habits.  
The set of cleaning tasks necessary for each of these data sources is highly domain and application specific,
and oftentimes the developer is concurrently trying to clean the data source as well as understand its properties.


Oftentimes, these data sources have data quality issues that require a complex data cleaning pipeline -- 
e.g., data extraction, re-formatting, identification and fixes of missing or incorrect values,
and removal of redundant information -- before the data is useable by downstream processes.
Data sources are often domain specific and new for the data analyst, 
As datasets continue to grow, and organizations make use of mure and more datasets, the ability to
rapidly clean the data is more important.  
\fi
