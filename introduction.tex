% !TEX root = demo.tex
\section{Introduction}\label{sec:intro}

\begin{figure}[t]
\centering
\vspace{-0.5cm}
\includegraphics[width = .4\textwidth]{figs/lifecycle.png}
\vspace{-0.2cm}
\caption{Example iterations of the extraction portion of a cleaning pipeline that extracts restaurant information from webpages.  1) User uses a sample to decide if there are enough unique restaurants in the webpages to be worth cleaning.
2) User then runs the rule-based extractor on the whole dataset and inspects the live results.  2) The quality was low, so user moves to a manual human-based solution.  3) The quality is high, but the crowd-based extractor is 
slow. There are enough high quality extractions to train a classifier that decides if the output of the automated process should be sent to the crowd.}
\label{fig:ex-plan}
\end{figure}
% why feedback loops exist/are important (cite Joe’s shit in the past 3 years, blinkdb, etc.). highlight domain specificity. Large variety of data cleaning tasks

%
% Genereal comment that data cleaning is important:
%
The prevalence of dirty data presents a fundamental obstacle to modern data-driven applications, since 
blindly using results that are derived from dirty data can lead to hidden, yet significant errors.
Analysts report spending upwards of 80\% of their time on problems in data cleaning \cite{kandel2012} including 
error diagnosis, instance-specific cleaning scripts, and managing large data ingest pipelines.
The underlying problem is that data cleaning is often specific to the domain, dataset, and eventual analysis,
leading to a breadth of possible errors that are manifest in the data and a variety of options to clean it.
The user must go through the cleaning process with little or no guidance, deciding for each of her data sources what to extract, how to clean it,
and whether that cleaning will significantly change results. 
Without a general, user-efficient, and scalable data cleaning system, analysts have to repeatedly manually construct data cleaning workflows while simultaneously managing a very complex tradeoff space.

Figure~\ref{fig:ex-plan} shows a data cleaning workflow that prepares a dataset of unstructured restaurant webpages for analysis.
In order to run queries over the restaurants, the data analyst must extract structured fields of interest (e.g. address, cuisine category), then eliminate other sources of error, such as duplicate restaurant entries or non-canonical cuisine categories (e.g., `Chinese' vs. `Chinese food'). 
Though these data cleaning requirements have been specified at a logical level, there is a huge space of possible \textit{physical} implementations of the logical operators. 
For example, extraction could be rule-based, rely on structured learning, ask crowd workers to extract the desired data fields, or some combination of all three.
Even after selecting (say) a crowd-based operator, many parameters might influence the quality of the output data or the speed and cost of cleaning it: the number of crowd workers who vote on the extraction for a given webpage, the amount each worker is paid, etc.
A priori, a data analyst has little intuition for what physical plan will be optimal in this large space. 
In practice, she will likely pick a simple default for each logical operator (say, a rule-based extraction), then run data through the workflow and iterate by changing parameters or choosing new physical operators until she is satisfied with the cleaned data.

%The prevalance of data cleaning systems in both the research and industrial communities --
%Corleone does blah, XXX addresses blah. Nadeef does blah -- speak to the importance of a
%data cleaning framework as part of the modern big data ecosystems. \ewu{include open access of data in argument?} \jn{We also need to take a look at data cleaning systems in industry. }


% why existing systems suck aka related work
% 1. have slow feedback loops (dataset-dependence, …)
% 2. solve very specific data-cleaning tasks
Since the beginning of data management, systems have been explored by both the research and industrial communities to improve data cleaning efficiency and quality.
However, no existing systems address the end-to-end data cleaning process described above.
Extract-transform-load (ETL) systems~\cite{informatica,talend,apachefalcon} require developers to manually write data cleaning rules and execute them as long batch jobs, 
and constraint-driven tools that allow analysts to define ``data quality rules" and automatically propose corrections to maximally satisfy these rules \cite{DBLP:conf/sigmod/DallachiesaEEEIOT13}.
Unfortunately, neither provide the opportunity for iteration or user feedback, inhibiting the user's ability to rapidly prototype different data cleaning solutions.
Projects such as Wrangler~\cite{wrangler,trifacta} and OpenRefine~\cite{openrefine} support iteration with user-centric spreadsheet-style interfaces that enable the user to inspect a sample of the dataset, compose data cleaning sequences using a direct manipulation interface, and apply these sequences to the full dataset.
However, they are limited to specific extraction-based cleaning tasks, do not support crowd or machine learning-based refinement at scale, and cannot incorporate user feedback to optimize the data cleaning sequences.
Crowd-based~\cite{gokhale2014corleone,stonebraker2013data} systems have been proposed to relieve the data cleaning analyst of the burden of rule specification or manual cleaning, but are usually very task specific (e.g., de-duplication~\cite{gokhale2014corleone,park2014crowdfill,eracer,chen2014integrating}) and require analysts to awkwardly chain systems together to execute the entire cleaning workflow, preventing end-to-end optimization of the entire plan.
These existing limitations suggest the need for a system that is general enough to adapt to a wide range of data cleaning applications, scales to large datasets, and natively supports fast-feedback interactions to enable rapid data cleaning iteration.

In this paper, we introduce \sys, a system designed to support such a workflow end to end.
\sys allows users to specify declarative data cleaning plans composed of rule-based, learning-based, or crowd-based operators, then makes cost-aware recommendations for improving the accuracy or latency of a plan by swapping in new physical operators or modifying their parameters.
\sys supports rapid iteration on plans by providing users with the opportunity to provide ground-truth feedback after each logical operator in the plan, improving recommendations, and provides efficient support for making adjustments to in-flight plans using caching and tuple lineage.

Supporting these capabilities requires a mixture of careful engineering 
as well as tackling several research challenges:

\squishlist
\item {\bf Operator and API design}: We have designed a core set of logical data cleaning operators --
filter, similarity join, extract and sample -- that can be composed to
implement high-level cleaning tasks such as entity resolution.  Operators support
rule-based, learning-based, and crowd-based physical implementations. 
To support crowd-based cleaning operators, we have developed a crowd-managment 
framework that delivers data cleaning tasks to multiple crowdsourcing platforms using a common API.

\item {\bf Near-instant feedback}: We accelerate feedback loops in the system with techniques to reduce both 
intra- and inter-operator latencies. To improve Crowd operator performance, we leverage techniques such as straggler 
mitigation~\cite{venkataraman2014power}, worker latency modeling, and bandit algorithms~\cite{thompson1933likelihood}
to consistently retrieve crowd results in seconds rather than hours. To reduce latencies at a plan level, we allow data cleaning to 
proceed asynchronously and allow users to introspect and modify data cleaning plans at any point during their execution.

\item {\bf Hot Swapping}: In contrast to batch-oriented data cleaning, rapid iteration necessitates the
ability to tune an operator parameter or reconfigure the sequence of cleaning operators as the cleaning
plan is running.  We develop techniques that leverage tuple lineage to intelligently cache intermediate results to minimize
re-computation when an in-flight plan has been modified.
\squishend


In our demonstration, we will run an entity resolution plan on two restaurant datasets, and
show how \sys can be used to 1) specify and execute a data cleaning plan using our domain specific
language, 2) quickly clean a sample to characterize how a plan is performing, 
3) observe that cleaning plans are not necessarily optimal across datasets, and 
4) incrementally refine the plan to fit a new dataset by changing an operator's parameter (e.g., similarity threshold)
or its physical implementation.
The fourth interaction lets the participants inspect the effects of different physical plans.
Users can then execute a plan over a live crowd that uses the audience as workers, or a simulated crowd
that uses pre-collected crowd responses. The dashboard (Figure~\ref{screenshot-rec}) also provides a live inspection
interface to view the status of the cleaning plan as it executes.



%Our contributions/requirements
%different ways to tighten the feedback loop:
%end-to-end latency/cost (operator optimization)
%looking versus touching
%Adding introspection (more points of observation)
%hot-swapping (more points of changing plans)
%We have built an end-to-end data cleaning framework with these requirements in mind. (... things we do …) (... engineering contributions …).
%In this demonstration, we highlight the benefits of improving feedback loops for data cleaning using X datasets by optimizing a data cleaning pipeline for one data set/cleaning task, then quickly fitting the pipeline to another dataset.


\if{0}

\jn{Honestly, I didn't quite buy declarativity of the system. In my opinion, data cleaning is so domain specific. It's hard to make it declarative. For a given domain, people may need to write their own data cleaning system. There is a lack of a data cleaning framework that they can build based on. This motivates us to develop such framework. 


We analyze a large variety of domain specific data cleaning systems, and identify several key components: declarative data cleaning operators (e.g., similarity joins), active learning, and crowd/expert sourcing platforms that they require. In our framework, we abstract these components, and implement them in a general way. 

We mainly address two challenges: extensibility and scalability. For the former one, we came up with a nice data-cleaning pipeline API, which people can easily use to compose their own data cleaning tasks. For the latter one, we address it in two aspects: Sampling + Asynchrony.}

\ewu{That's fair, will need to address why a framework is necessary and what benefits it provides.  I think a framework is the correct pitch, hard to sell a set of operators.  Are the above challenges -- extensibility and scalability -- actually difficult?  Worried it's straightforward application of existing techniques.}



In contrast, our work is based on the observation that the majority of data cleaning workflows
can be decomposed into a small set of logical operations (in addition to traditional database operators):
filter based on constraints, extract new fields from existing data, and a similarity join to match
similar or duplicate records. \ewu{quickly validate why this observation holds.} \jn{Yes! I also found that Sec 2.3 has more operators than you describe here.}  
By designing a system around these core operators, we can provide a vast library of physical  
data cleaning operators that span the range of algorithmic, machine learning, and human computation-based
implementations that are necessary practical data cleaning pipelines.   \ewu{Describe live inspection as 
a core feature or is it too easy?}

Designing such a system requires tackling several design challenges:

\begin{enumerate}
\item Speed
\item Quality
\item API Design/extensibilty
\end{enumerate}



We have implemented an initial version of \sys on top of the AMPLab Spark stack, which provides us 
with access to its advanced distributed processing and machine learning features.  Our goal for the current
version is to implement the core mechanisms for declarative specification of the
data cleaning pipeline, solidify the API design, and incle support for, and implementations of,
multiple classes of physical data cleaning operators.


\fi



\if{0}

Cleaning, pre-processing, and formatting data is a required first step in any data analytics pipeline.
However, despite this importance, large-scale data analytics platforms such as Spark or Hadoop lack integrated data cleaning frameworks.
There are a few challenges in building a general purpose data cleaning framework: (1) data cleaning is often
domain specific and requires specialized software targeted at one or a handful of data sources \cite{wang1999sample}, (2) data cleaning is often 
expensive as it increasingly involves human effort via crowdsourcing or experts \cite{DBLP:conf/sigmod/GokhaleDDNRSZ14}, and (3) learning how to clean dirty data from examples
is often hard without a greatly restricted set of operators \cite{DBLP:conf/uist/GuoKHH11}.

We address this problem in \projx by designing a Spark library of composable and scalable data cleaning primitives.
\projx abstracts the logical data cleaning operators: Extraction, Similarity Join, Filtering, from the physical implementation i.e, Rule, Crowd, or Machine Learning.
We interface these primitives through a DSL with which a user can build data cleaning operators that suit their needs.
\projx provides transparent optimizations for each of the components and their composition.
In this demonstration proposal, we present \projx and highlight some of its key features.
While there are many existing systems that do one aspect of data cleaning and transformation (e.g Entity Resolution or Extraction), 
many real world data cleaning tasks have multiple types of errors.
Composing disparate systems can lead to complex code and inefficiencies at scale.
With \projx, we hope to design a set of optimized composable primitives that span a large space of data cleaning tasks.

The first key feature of \projx is that it provides optimized distributed implementations 
of the physical data cleaning operators.
For example, a key step in many deduplication algorithms is a Similarity Join which finds all pairs of records that are within some similarity threshold.
A naive implementation of a Similarity Join would apply a similarity function to all pairs of records.
However, in \projx, we provide optimized implementations of certain common similarity functions (e.g Jaccard, Edit Distance, etc.) that allow for 
a combined broadcast join and prefix filtering which intelligently skips pairs of records using a broadcasted inverted index.

Another feature of \projx is managing the latency and the scale problems of crowd-based data cleaning. 
Crowdsourcing is increasingly prevalent in data cleaning, and \projx provides physical crowd-based implementations
of the logical operators.
However, crowds work at a different latency and scale point in comparison to distributed analytics platforms.
To address the latency problem, we build asychrony into the system.
The user can query intermediate results at any time as crowd responses stream in.
To address the scale issue, \projx provides sampling primitives.
The glue that ties all of the crowd components together is a Machine Learning technique called Active Learning.
As we collect more and more crowd responses, we learn a model that predicts these responses to apply it on the uncleaned data.
Active Learning selects the most informative questions to ask the crowd.

Finally, \projx provides an approximate query processing (AQP) framework.
With slow asynchronous data cleaning algorithms as in crowdsourcing, we need 
to define clear semantics for the intermediate results.
Our AQP framework uses the algorithms proposed in \cite{wang1999sample}, to estimate and bound early results.
It is also common for data scientists to prototype expensive data cleaning pipelines on samples and AQP allows quick evaluation of
aggregate query results on a cleaned sample.

\subsection{Demonstration Scenario}
\reminder{TODO}

\fi




\if{0}
Consider for example, the ability to rapidly understand the types of errors that are present, as well as prevalance of 
these errors is cruicial.



Before an organization can use a new dataset as part of their analysis pipeline
(e.g., to build complex learning models or answer analyst queries)
the errors in the dataset need to be removed in order to ensure accurate conclusions.  

Modern data-driven organizations rely on the ability to ingest and generate large data sets from 
disparate sources, and combine the data together to build complex models or answer analytical questions.  
For example, a restaurant review website may collect restaurant listings by scraping data from webpages or purchasing them from external sources, and
restaurant visitation information for sources such as OpenTable or FourSquare, and aggregate the data to
model user eating habits.  
The set of cleaning tasks necessary for each of these data sources is highly domain and application specific,
and oftentimes the developer is concurrently trying to clean the data source as well as understand its properties.


Oftentimes, these data sources have data quality issues that require a complex data cleaning pipeline -- 
e.g., data extraction, re-formatting, identification and fixes of missing or incorrect values,
and removal of redundant information -- before the data is useable by downstream processes.
Data sources are often domain specific and new for the data analyst, 
As datasets continue to grow, and organizations make use of mure and more datasets, the ability to
rapidly clean the data is more important.  
\fi
