\section{Challenges}
In this section, we describe some of the research and engineering challenges in developing \sys.

\subsection{Research Challenges}
Data cleaning is an inherently iterative process where, through trial and error, users optimize a data cleaning workflow and parameters for a specific dataset.
Iterating over these workflows can be very time consuming and complex to debug (i.e., if one operator uses the crowd).
Therefore, in our research, we explore two categories of optimizations: (1) intra-operator optimizations and approximations that reduce the latency of individual operators, and (2) plan-level optimizations that allow for asynchrony and mid-execution modification.

\subsubsection{Reducing Operator Latency}


\vspace{.5em}

{\noindent \bf Sampling:} One way to reduce operator latency is to reduce the number of tuples each operator processes.
In \sys, sampling is a logical operator which is an invaluable tool in developing and iterating on data cleaning plans.
\sys allows for quick prototyping and optimizing workflows on samples of data and then transfering these optimizations to full datasets.
Sampling allows for quick introspection of otherwise opaque components, such as testing the quality of a crowd with a small set of records.
\sys also provides the statistical tools to extrapolate (with confidence intervals) the insights learned on a sample.
The problem of estimating aggregate query results has been proposed in prior work ~\cite{wang1999sample} and is challenging since data cleaning may change
the statistics of a sample making it not uniform.
In this work, we build on these results to allow users to declare aggregate queries of interest and notify them when a change to a plan has a statistically significant effect.

%Recently proposed, sample-based query processing methods for dirty data can help inform users the changes about the statistical significance of 
%changes made to the data.
%For example, suppose one collects a restaurant dataset, and runs some aggregation queries on the dataset, e.g., computing the number of the restaurants in San Fransisco and getting a query result of 12000. 
%But, when looking at the dataset, she finds that there are many duplicate restaurants in the dataset, and some restaurants' locations are misspelled (e.g., S\underline{e}n Fransisco). She can use our system to create a sample of the data, and then apply a data cleaning procedure to the sample. 
%After the sample is cleaned, our system can estimate the impact of data cleaning on her original query results, and return her corrected answers with confidence intervals, e.g., $5000\pm300$. Intuitively, this result means that if the same data-cleaning procedure is applied to the full dataset, the number of the restaurants in San Fransisco will be within $[4700, 5300]$. 

\vspace{.5em}
{\noindent \bf Crowdsourcing and Active Learning:} Crowd-based operators are often the rate-determining steps in a data cleaning plan.
A key problem is that completion time of the operator largely depends on how fast all of the tasks can be completed by some crowd worker.
This method can be very slow since in a real-world crowdsourcing platform, it is very likely that there are some workers who do tasks much slower than others; analagous to the straggler problem in distributed systems.
We address this problem by maintaining a pool of crowd workers and keeping track of the characteristics (i.e., quality and speed) of each worker in the pool. We develop a task routing strategy that can avoid assigning tasks to slow workers and significantly reduce the time that is required to train a machine learning model from the crowd.

%Even cleaning a sample of data, data cleaning can still take a lot of time. This is especially true when it requires humans to clean the data. 

\subsubsection{Plan-level Optimization}
The next category of optimizations happen at a plan-level i.e., the sequence or execution pattern of the operators.
We highlight two challenges: allowing users to see early results, and allowing users to change data cleaning plans mid-execution.

\vspace{.5em}
{\noindent \bf Asynchronous Data Cleaning:} To provide more fine-grained feedbacks during a data cleaning process, our system allows users to execute a data cleaning operation asynchronously. Tuples are processed by each of the operators in the plan and the intermediate results are persisted allowing users to query the results at any time. Thus, at any time, the user can get a ``best effort" query result.

\vspace{.5em}

The challenge is that these pipelines can be very long running operations, and even with sampling, it is inefficient to restart the plan every time the user wants to make a small change.
We design a framework for \emph{hot swapping} plan operators and re-using exisitng results through caching and lineage, which handle downstream and upstream changes respectively.
The challenge is to do this while preserving correctness of the operation by ensuring that the result is exactly the same as if the modified plan was run from the start.
Of course, this requires users to conform to an API so that the input and output behaviors of the operators are the same.


\vspace{.5em}

{\noindent \bf Hot Swapping (Caching): } Caching allows for result re-use if a downstream operator is modified or added.
If the system has sufficient memory, then we can cache all of the intermediate results. 
However this is not always possible, and the key challenge is to select which results to cache.
To do this, we have to integrate the caching framework with our recommendation engine.
When, we make a recommendation for a change, we must cache the preceding operator. 

\vspace{.5em}

{\noindent \bf Hot Swapping (Lineage): } We can analyze the
semantics of a modified operator to understand how results change, \i.e. decreasing a similarity join threshold increases the number of output pairs or adding an additional filtering step reduces the number of output tuples. 
The key property here is monotonicity, and some types of monotone \textsf{Filter}, and \textsf{SimilarityJoin} are data cleaning analogs for a Select-Join relational algebra.
In this setting, can model this problem as a view/lineage problem.
Since they are monotone, we can perform a variant of incremental maintenance updating a final result given insertion or deletion of intermediate tuples.

\iffalse
\subsection{Research Challenges}

Architecturally, we separate crowd sourcing and automated data cleaning.
There is a crowd server that acts as a layer of indirection between the Spark codebase and crowd sourcing APIs.

\team{Describe Research Challenges.  To what extend can we actually do
optimization given the physical operators?  Unlike relalgebra, the physical operators here are
not interchangable!}



\subsection{Learning Parameters From Example}
In simple cases, it might be easy to use domain knowledge to select and tune physical operators. 
In more complex cases, it might be easier to specify a sample of dirty and clean data instead of the function.
It may also not be feasible to have the crowd clean the entire dataset.
In these cases, we want to learn a statistical model from which we can extrapolate those responses to the rest 
of the data.

In our current implementation of \projx, we pose this learning problem as classification problems.
In general, these parameter functions can be quite complex and this is a simplification of the learning problem.
The choice of classifier and featurization is upto the user. 
We currently support Support Vector Machines and Decision Trees with a featurization library that includes common text processing features.

\vspace{0.5em}

\noindent \textsf{Filter(R, $\mathcal{T}^+$, $\mathcal{T}^-$)}: Given a set of positive training examples $\mathcal{T}^+$ (i.e, $r$ that satisfy the condition) and
negative training examples $\mathcal{T}^-$ (i.e, r that do not satisfy the condition), we learn a classifier that predicts whether a record satisfies the condition. 

\vspace{0.5em}

\noindent \textsf{Extract(R, a, $\mathcal{T}$)} We restrict the learned problem setting to delimited extraction. Given a set of training examples $R(a)$ and the output $v_1,v_2,...,v_k$, we learn a classifier to predict which characters in $R(a)$ are delimiters based on the tokens in the string.

\vspace{1em}

To acquire the samples of clean data, uniform sampling may not be the best strategy.
For example, if there examples of dirty data are very rare, we will not be able to learn a model.
We implement a technique called Active Learning to sample.
Active Learning selects the most informative examples based on the current model so far.
We use an Active Learning algorithm called uncertainty sampling to do this.




\subsection{Inspection}

Data cleaning requires inspection -- user wants to see how the dataset
has been "cleaned"  through the pipeline.  What does that even mean?

\subsection{Dynamic Re-optimization}

Crowd means want to swap in or out operators at run time.


\subsection{Lineage}

\noindent\textbf{Lineage: }
We track the lineage of rows using a primary key.
Users are not allowed to modify this primary key with any operations.
This allows us to apply operations like transitive closure even after projection since we have a unique identifier for each row.

\vspace{0.5em}
\noindent \textbf{Example: } Suppose, we are interested in deduplication of unstructured data. Then, we could apply the following logical operations.
We first apply an \textsf{Extract} operation to extract the unstructured data into columns. If some of the columns are inconsistent in their representation,
we apply \textsf{Project} to those columns that are inconsistent. We can then take a \textsf{SimilarityJoin} to group rows that are similar, and finally
we resolve those differences with \textsf{TransitiveClosure}.
\fi


